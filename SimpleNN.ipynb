{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4db36ffe",
   "metadata": {},
   "source": [
    "SimpleNN XOR Demo\n",
    "The purpose of this demo is to show how a neural network built from scratch \n",
    "works and to prove that the neural network built from scratch is extensible. \n",
    "I have created this toy for the purpose of understanding how neural networks \n",
    "work with a reasonable level of understanding.  This toy is not an effectively\n",
    "scalable network intended for professional use.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35068776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug:\n",
      "[[-0.037676693354512514, 0.8866797406299332], [0.5037455986676163, 0.32036358318478886]]\n",
      "[0.023358298131871985, 0.08358894870854247]\n",
      "[[-0.05277384722807646], [0.02619150736007031]]\n",
      "[0.07172226942346058]\n",
      "Final training output: [[0.0017406713099263277], [0.9959394337244654], [0.9959343240644504], [0.0019854050437767743]]\n",
      "We were training for this output: [[0.0], [1.0], [1.0], [0.0]]\n"
     ]
    }
   ],
   "source": [
    "from SimpleNN import SimpleNN\n",
    "from math import tanh\n",
    "\n",
    "# SimpleNN takes its training data in its constructor.  Here, we pass in an \n",
    "# array of input parameters and an array of expected outputs to train for. For\n",
    "# XOR, we're passing in the values for: \n",
    "# (0,0) => 0\n",
    "# (0,1) => 1\n",
    "# (1,0) => 1\n",
    "# (1,1) => 0\n",
    "xorNN = SimpleNN([[0.0,0.0], [0.0,1.0], [1.0,0.0], [1.0,1.0]],[[0.0],[1.0],[1.0],[0.0]])\n",
    "\n",
    "# Set the tanh function and its derivative for the activation and activation \n",
    "# prime functions\n",
    "xorNN.activation = tanh\n",
    "xorNN.activationPrime = lambda x:1 - tanh(x)**2\n",
    "\n",
    "# Cycle through forward and back propagation until our outputs are under a \n",
    "# certain loss when compared with our expected outputs. \n",
    "xorNN.train()\n",
    "\n",
    "# Print the final output vector: \n",
    "print(f\"Final training output: {xorNN.actualOutput}\")\n",
    "print(f\"We were training for this output: {xorNN.expectedOutput}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
